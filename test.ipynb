{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\"https://docs.google.com/spreadsheets/d/1OReP9PTU8p6VNrGRX1ePl0D6h9mhDMy1bO8B8pRjhzM/edit?usp=sharing\"\n",
    "def convert_google_sheet_url(url):\n",
    "    # Regular expression to match and capture the necessary part of the URL\n",
    "    pattern = r'https://docs\\.google\\.com/spreadsheets/d/([a-zA-Z0-9-_]+)(/edit#gid=(\\d+)|/edit.*)?'\n",
    "\n",
    "    # Replace function to construct the new URL for CSV export\n",
    "    # If gid is present in the URL, it includes it in the export URL, otherwise, it's omitted\n",
    "    replacement = lambda m: f'https://docs.google.com/spreadsheets/d/{m.group(1)}/export?' + (f'gid={m.group(3)}&' if m.group(3) else '') + 'format=csv'\n",
    "\n",
    "    # Replace using regex\n",
    "    new_url = re.sub(pattern, replacement, url)\n",
    "\n",
    "    return new_url\n",
    "\n",
    "# Example usage:\n",
    "url = 'https://docs.google.com/spreadsheets/d/1OReP9PTU8p6VNrGRX1ePl0D6h9mhDMy1bO8B8pRjhzM/edit?usp=sharing'\n",
    "new_url = convert_google_sheet_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.google.com/spreadsheets/d/1OReP9PTU8p6VNrGRX1ePl0D6h9mhDMy1bO8B8pRjhzM/export?format=csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(new_url)\n",
    "# https://docs.google.com/spreadsheets/d/1mSEJtzy5L0nuIMRlY9rYdC5s899Ptu2gdMJcIalr5pg/export?gid=1606352415&format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hoeveel jaar garantie heeft de stoel: de stoel heeft 5 jaar garantie', metadata={'Product': 'Stane Classic Basic', 'EAN': 8719327684375, 'Question': 'Hoeveel jaar garantie heeft de stoel', 'Answer': 'de stoel heeft 5 jaar garantie'}),\n",
       " Document(page_content='Hoeveel jaar garantie heeft de plus versie?: De plus versie heeft 6 jaar garantie', metadata={'Product': 'Stane Classic Plus', 'EAN': 8719327684337, 'Question': 'Hoeveel jaar garantie heeft de plus versie?', 'Answer': 'De plus versie heeft 6 jaar garantie'}),\n",
       " Document(page_content=\"What's price of baseball bat: 20$\", metadata={'Product': 'baseball bat', 'EAN': 1231024012041014, 'Question': \"What's price of baseball bat\", 'Answer': '20$'}),\n",
       " Document(page_content=\"what's material of baseball bat: wood\", metadata={'Product': 'baseball bat', 'EAN': 1231024012041014, 'Question': \"what's material of baseball bat\", 'Answer': 'wood'}),\n",
       " Document(page_content='how many types of baseball bat: 3', metadata={'Product': 'baseball bat', 'EAN': 1231024012041014, 'Question': 'how many types of baseball bat', 'Answer': '3'}),\n",
       " Document(page_content=\"What's the price of pair of Nike shoes?: 10$\", metadata={'Product': 'Nike Shoes', 'EAN': 1231231312, 'Question': \"What's the price of pair of Nike shoes?\", 'Answer': '10$'}),\n",
       " Document(page_content=\"What's material of Nike Shoes?: Wool\", metadata={'Product': 'Nike Shoes', 'EAN': 1231231312, 'Question': \"What's material of Nike Shoes?\", 'Answer': 'Wool'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "\n",
    "df = pd.read_csv(new_url)\n",
    "df['text'] = df['Question'] + ': ' + df['Answer']\n",
    "loader = DataFrameLoader(df, page_content_column='text')\n",
    "documents = loader.load()\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'docs/chroma/'\n",
    "!rm -rf ./docs/chroma  # remove old database files if any\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(documents, \n",
    "                           embedding_function, \n",
    "                           persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"What's the price of pair of Nike shoes?: 10$\", metadata={'Answer': '10$', 'EAN': 1231231312, 'Product': 'Nike Shoes', 'Question': \"What's the price of pair of Nike shoes?\"}),\n",
       " Document(page_content=\"What's material of Nike Shoes?: Wool\", metadata={'Answer': 'Wool', 'EAN': 1231231312, 'Product': 'Nike Shoes', 'Question': \"What's material of Nike Shoes?\"})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "query = \"price Nike Shoes?\"\n",
    "docs = db.max_marginal_relevance_search(query,k=2, fetch_k=3)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type='mmr', k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "openai_api_key=\"\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        temperature=0.5, model=\"gpt-4\", openai_api_key=openai_api_key, verbose=True\n",
    "    )\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "PREFIX=\"\"\"You are a product analyst that will give the answer based on the provided context only. \n",
    "Please reply with the user's language. If user speaks Dutch, reply with Dutch\n",
    "If you cannot find the information, must reply I don't know in the user's language.\"\"\"\n",
    "\n",
    "template = PREFIX + \"\"\"\n",
    "The context provided:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(template=template, input_variables=[\"context\" ,\"question\"])\n",
    "\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_type='mmr', k=3, fetch_k=5),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT, \"verbose\": True},\n",
    "    verbose=True,\n",
    ")\n",
    "result = qa_chain({\"query\": query})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f8f0627fb50>, search_type='mmr')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.as_retriever(search_type='mmr', k=3, fetch_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "PREFIX=\"\"\"You are a product analyst that will give the answer based on the provided context only. \n",
    "Please reply with the user's language. If user speaks Dutch, reply with Dutch\n",
    "If you cannot find the information, must reply I don't know in the user's language.\"\"\"\n",
    "\n",
    "template = PREFIX + \"\"\"\n",
    "The context provided:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(template=template, input_variables=[\"context\" ,\"question\"])\n",
    "\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_type='mmr', k=3, fetch_k=5),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT, \"verbose\": True},\n",
    "    verbose=True,\n",
    ")\n",
    "result = qa_chain({\"query\": query})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hoeveel jaar garantie heeft de plus versie?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 7, updating n_results = 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a product analyst that will give the answer based on the provided context only. \n",
      "Please reply with the user's language. If user speaks Dutch, reply with Dutch\n",
      "If you cannot find the information, must reply I don't know in the user's language.\n",
      "The context provided:\n",
      "What's the price of pair of Nike shoes?: 10$\n",
      "\n",
      "What's material of Nike Shoes?: Wool\n",
      "\n",
      "Hoeveel jaar garantie heeft de stoel: de stoel heeft 5 jaar garantie\n",
      "\n",
      "how many types of baseball bat: 3\n",
      "\n",
      "Question: gía giày adidas?\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tôi không biết'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jimmy/miniconda3/envs/deep-learning-ws/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Justin Beiber was born on March 1, 1994. The Super Bowl is typically played in early February, so we need to look at the Super Bowl that occurred in 1994. \\n\\nIn 1994, the Super Bowl was Super Bowl XXVIII, which was played on January 30, 1994. The teams that played in this Super Bowl were the Dallas Cowboys and the Buffalo Bills. \\n\\nThe Dallas Cowboys won the Super Bowl, defeating the Buffalo Bills by a score of 30-13. Therefore, the Dallas Cowboys won the Super Bowl in the year Justin Beiber was born.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent, create_csv_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "PREFIX=\"You are a product analyst with the information given in the sheet below. You will give the answer based on the information in the sheet only. If you cannot find the information, say I don't know.\"\n",
    "\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n",
    "    df,\n",
    "    prefix=PREFIX,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")\n",
    "\n",
    "agent_csv = create_csv_agent(\n",
    "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n",
    "    new_url,\n",
    "    prefix=PREFIX,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mDe EAN 8719327684337 heeft 6 jaar garantie.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'De EAN 8719327684337 heeft 6 jaar garantie.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_csv.run(\"Hoeveel jaar garantie heeft ean 8719327684337?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mDe plus versie heeft 6 jaar garantie.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'De plus versie heeft 6 jaar garantie.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Hoeveel jaar garantie heeft de plus versie?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
